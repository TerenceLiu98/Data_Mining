---
title: "Association Analysis: Basic Concepts and Algorithms"
author: "Terence Lau"
date: "4/13/2019"
output: pdf_document
---
## Motivation

Many business enterprises accumulate large quantities of data from their day-to-day operations. For example, huge amounts of customer purchase data are collected daily at the checkout counters of grocery stores. This such data, commonly known as market basket transactions and here is a example:

  TID 	ITEMS                       
  1   	{Bread, Milk}               
  2   	{Bread, Diapers, Beer, Eggs}
  3   	{Milk, Diapers, Beer, Cola} 
  4   	{Bread, Milk, Diapers, Beer}
  5   	{Bread, Milk, Diapers, Cola}

Each row in this table corresponds to a transaction, which contains a unique identifier labeled TID and a set of items bought by a given customer. The retailers are interested in the customers' purchasing behavior. This chapter presents a methodology known as association analysis, which is useful for discovering interesting relationships hidden in large data sets. The uncovered relationships can be represented in the form of association rules or sets of frequent items.

## Definition: Frequent Itemset

Using the example above:

  TID 	ITEMS                       
  1   	{Bread, Milk}               
  2   	{Bread, Diapers, Beer, Eggs}
  3   	{Milk, Diapers, Beer, Cola} 
  4   	{Bread, Milk, Diapers, Beer}
  5   	{Bread, Milk, Diapers, Cola}

- Itemset is a collection of one or more items: {Bread, Milk}
  - k-itemset: An itemset that contains k items
- Support count($\sigma$)
  - Frequency of occurrence of an itemset
  - E.g. $\sigma(\{\text{Milk, Bread, Diaper}\}) = 2$
- Support
  - Fraction of transaction that contain an itemset
  - E.g. $s(\{\text{Milk, Bread, Diaper}\}) = \frac{2}{5}$
- Frequent Itemset
  - An itemset whose support is greater than or equal to a minsup threshold

An important property of an itemset is its support count, which refers to the number of transactions that contain a particular itemset. Mahtematically, the support count, $\sigma(X)$, for an itemset X can be stated as follows:
 $\sigma(X) = |\{t_i|X \subseteq t_i, t_i \in T\}|$

## Definition: Association Rule

An association rule is an implication expression of the form $X \longrightarrow Y$, where X and Y are disjoint itemsets, i.e., X \cap Y = \emptyset

- Example: {Milk, Diaper} \rightarrow {Beer}
The strength of an association rule can be measured in terms of its support and confidence. Support determines how often a rule is applicable to a given data set, while confidence determines how frequently item in Y appear in transactions that contain X. The formal definiation of these metrics are:

$$\begin{align*}
\text{Support, } s(X \rightarrow Y) &= \frac{\sigma(X \cup Y)}{N}\\
\text{Confidence, } c(X \rightarrow Y) &= \frac{\sigma(X \cup Y)}{\sigma(X)}
\end{align*}$$

In statistical way to comprehend these formulas, in my opinion, the support is the probability of $X \cup Y$ and confidence is the conditional probability of Y given X, is $P\{Y|X\} = \frac{P\{X \cup Y\}}{P(X)}$

Example
Consider the rule {Milk, Diapers} $\rightarrow$ {Beer}. Since the support count for {Milk, Diapers, Beer} is 2 and the total number of transactions is 5, the rule's support is $\frac{2}{5} = 0.4$. The rule's confidence is obtained by dividing the support count for {Milk, Diapers, Beers} by the support count for {Milk, Diapers}. Since there are 3 transactions that contain milk and diapers, the confidence for this rules is $\frac{2}{3} = 0.67$

## Why We Use Support and Confidence?

Support is an important measure because a rule that has very low support may occur simply by chance. A low support rule is also likely to be uninteresting from a business perspective because it may not be profitable to promote itmes that customers seldom by together.

Confidence, on the other hand, measures the reliability of the inference made by a rule. For a given rule $X \rightarrow Y$, the higher the confidence, the more likely it is for Y to be present in transactions that contain X. Confidence also provides an estimate of the conditional probability of Y given X.

## Definition: Association Rule Discovery

Given a set of transaction T, find all the rules having support $\geq$ minsup and confidence $\geq$ minconf, where minsup and minconf are the corresponding support and confidence thresholds.

A brute-force approach for mining association rules is to compute the support and confidence for every possible rule. This approach is prohibitively from a data set. More specifically, the total number of possible rules extracted from a data set that contains d items is
 $R = 3^d - 2^{d + 1} + 1$

- List all possible association rules
- Compute the support and confidence for each rule
- Prune rules that fail the minsup and minconf thresholds

$\Rightarrow Computationally prohibitive$

Therefore, a common strategy adopted by many association rule mining algorithms is to decompose the problem into two major subtasks:

1. Frequent Itemset Generation, whose objective is to find all the itemsets that satisfy the minsup thresold. These itemsets are called frequent itemsets.
2. Rule Genreation, whose objective is to extract all the high-confidence rules from the frequent itemsets found in the previous step. These rules are called strong rules.

## Frequent Itemset Generation

A lattice structure can be used to enumerate the list of all possible itemsets.

![Screenshot 2019-04-10 at 00.21.40.png](https://i.loli.net/2019/04/10/5cacc6988c899.png)

This figure shows an itemset lattice for I = $\{a, b, c, d, e\}$. In general, a data set taht contains k items can potentially generate up to 2^k - 1 frequent itemsets, excluding the null set. Because k can be very large in many practical applications, the search space of itemsets that need to be explored is exponentially large.

- brute-force approach for finding frequent itemsets is to determine the support count for every candidate itemset in the lattice structure. To do this, we need to compare each candidate against every transaction, an option that is shown below.
  ![Screenshot 2019-04-10 at 00.29.10.png](https://i.loli.net/2019/04/10/5cacc85d2cd82.png)
  
1. Reduce the number of candidate itemsets(M). The Apriori principle, describled in the next section, is an effective way to eliminate some of the candidate itemsets without counting their support values.
2. Reduce the number of comparisions. Instead of matching each candidate itemset against every transaction, we can reduce the number of comparisons by using more advanced data structures, either to store the candidate itemsets or to compress the data set.

The Complexity $\sim O(NMw) \Rightarrow$ Expensive since $M = 2^d$

## The Apriori Principle

This section describes how the support measure helps to reduce the number of candidate itemsets explored during frequent itemset generation. The use of support for pruning candidate itemsets is guided by the following principle.

Definition (Apriori Principle)

If an itemset is frequent, then all of its subset must also be frequent.

Suppose $\{c, d, e\}$ is a frequent itemset. Clearly, any transaction that contains $\{c, d, e\}$ must also contain its subset, $\{c,d\}, \{c, e\}, \{d, e\}, \dots$. As a result, if $\{c, d, e\}$ is frequent, than all subsets of \{c, d, e\} must also be frequent.

Conversely, if an itemset such as $\{a, b\}$ is infrequent, then all of its supersets must be infrequent too.

This strategy of trimming the exponential search space based on the support measure is known as support-based pruning. Such a pruning strategy is made possible by a key property of the support measure, namely, that the support for an itemset never exceeds the support for its subsets. This property is also known as the anti-monotone property of the support measure.

Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length k from item sets of length $k - 1$. Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent k-length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.

Frequent Itemset Generation in the Apriori Algorithm 

Apriori is the first association rule mining algorithm that pioneered the use of support-based pruning to systematically control the exponential growth of candidate itemsets. 



Initially, every item is considered as a candidate 1-itmeset. After counting their supports, the candidate itemsets {coke} and {Eggs} 

## Apriori Algorithm 

- Method: 
  - Let $k = 1$ 
  - Generate frequent itemsets of length 1 
  - Repeat until no new freqent itemsets are identified 
    - Generate length (k + 1) candidate itemsets from length k frequent itemsets 
    - Prune candidate itemsets containing subsets of length k that are infrequent 
    - Count the support of each candidate by scanning the DB 
    - Eliminate candidates that are frequent, leaving only those that are frequent 

## Important Details of Apriori 

* How to generate candidates? 
  * Step 1: self-joining $L_k$
  * Step 2: pruning 
* How to count supports of candidates?
* Example of Candidate-generation 
  * $L_3 = \{abc, abd, acd, ace, bcd\}$
  * Self-joining: $L_3 \times L_3$ 
    * $abcd$ from $abc$ and $abd$
    * $acde$ frome $acd$
  * Pruning: 
    * $acde$ is removed because $ade$ is not in $L_3$
    * $C_4 = \{abcd\}$
    
## Factors Affecting Complexity 

* Choice of minimum suppor threshold 
  * lowing support threshold results in more frequent itemsets 
  * this may increase number of candidates and max length of frequent itemsets 
* Dimensionality (number of items) of the data set 
  * more space is needed to sotre support count of each item 
  * if number of frequent items also increases, computation may increase 
* Size of database 
  * since Apriori makes multiple passes, run time of alogorithm may incease with number of transactions 
* Average transaction width 
  * transaction width increases with denser data sets 

## Rule Generation 

* Given a frequent item set $L$, find all non-emplty subsets $f \subset L$ such that $f \rightarrow L - f$ satisfies the minimum confidence requirement 

  * If $\{A, B, C, D\}$ is a frequent itemset, candidate rules: ![Screenshot 2019-04-13 at 20.07.36.png](https://i.loli.net/2019/04/13/5cb1d12bd51f4.png)
* If $|L| = k$, then there are $2^k - 2$ candidate association rules (ignoring $L \rightarrow \emptyset$ and $\emptyset \rightarrow L$)
How to efficient generate rules from frequent itemssets? 
  * In general, confidence does not have an anti-monotone property $c(ABC \rightarrow D)$ can be larger or smaller than $c(AB \rightarrow D)$ 
  * But confidence of rules generated from the same itemset has an anti-monotone property 
  * e.g., L = $\{A, B, C, D\}: c(ABC \rightarrow D) \geq c(AB \rightarrow CD) \geq c(A \rightarrow BCD)$

**The Apriori Method** 

Key ideas: 
  * A subset of a frequent items must be frequent.
  * The supsersets of any infrequent, itemset cannot be frequent
  
## Example 

In R, we first need two package: `arules` and `arulesViz`
```{R}
# install.packages("arules", dep = T)
library(arules)
```

Lets play with the Groceries data that comes with the arules pkg. Unlike dataframe, using head(Groceries) does not display the transaction items in the data. To view the transactions, use the inspect() function instead.

Since association mining deals with transactions, the data has to be converted to one of class transactions, made available in R through the arules pkg. This is a necessary step because the apriori() function accepts transactions data of class.

```{R}
# Groceries <- read.csv("https://raw.githubusercontent.com/TigerInnovate/PredictiveModeling/master/groceries.csv", header = T)
class(Groceries)

# we have to change the class of the data set into transaction 

trans_Gro <- as(Groceries, "transactions")
class(trans_Gro)
```
It is now a transactional dataset. 

There are three parameters controlling the number of rules to be generated *viz.* **Support and Confidence**. Another parameter **Lift** is generated using Support and Confidence and is one of the major paramters to filter the genreated rules.
* Support is an indication of how frequently the itemset appears in the dataset. Consider only the two transactions from the above output. The support of the item citrus fruit is 1/2 as it appears in only 1 out of the two transactions.
* Confidence is an indication of how often the rule has been found to be true. We will discuss more about confidence after generating the rules.

```{R}
grocery_rules <- apriori(trans_Gro, parameter = list(support = 0.1, confidence = 0.5))
inspect(head(sort(grocery_rules, by = "confidence"), 2))
```

In these codes, we can see the top 2 rules sorted by confidence. 



