---
title: "Classification"
author: "Terence Lau"
date: "4/14/2019"
output: pdf_document
---
## Classification 

### Definition 

* Given a collection of records (training set)
  * Each record contains a set of attributes, one of the attributes is the class. 
* Find a model for class attribute as a function of the values of oterh attributes. 
* Goal: previously unseen records should be assgined a class as accurately as possible 
  * A test set is used to determine the accuracy of the model. Usually, the given data set is divided into training and test sets. with training set used to build the model and test set used to validate it. 

![Screenshot 2019-04-14 at 19.40.59.png](https://i.loli.net/2019/04/14/5cb31c5111351.png)

### Classification Techniques 
  * Decision Tree based Methods 
  * Rule-based Methods 
  * Meoery based reasoning 
  * Neural Networks 
  * Naive Bayes and Bayesian Belief Networks 
  * Support Vector Machines 
  
## Classification with Decision Trees 

![Screenshot 2019-04-14 at 19.42.35.png](https://i.loli.net/2019/04/14/5cb31caea5e58.png)

Decision tree is a kind of tree induction algorithm
  * Partitions data into subsets based on categories of predictors 
  * Partitioning done to maximize predicatability of response variable
  * Uses statistical test to determine best partitioning/grouping of categories 
  * REsponse has two or more categories 
  
### Algorithm for Decision Tree Induction 
* Basic algorithm (a greedy algorithm)
  * Tree is constructed in a **top-down recursive divide-and-conquer manner**
  * At start, all the training examples are at the root 
  * Attributes are categorical (if continuous-valued, they are discretized in advance) 
  * Examples are partitioned recursively based on selected attributes 
  * Test attributes are selected on the basis of a heuristic or statistical measure (e.g. **information gain**)

* Conditions for stopping partitioning 
  * All samples for a given node belong to the same class 
  * There are no remaining attributes  for further partitioning - **majority voting** is employed for classifying the leaf 
  * THere are no samples left 
  
### Tree Induction 
**Greedy strategy** means we have ot splite the records based on an attribute test that optimizes certain criterion. But how? How to determine the best split? Here is some method. First, we need to introduce what is greedy approach? In greedy approach, nodes with **homogeneous** class distribution are preferred, also, we need to measure the node's impurity. For instance: if $\begin{align*} C_0 : \ 5 \\ C_1: \ 5 \end{align*}$ this is **non-homogeneous, and it has high degree of impurity**; however, if $\begin{align*} C_0 : \ 9 \\ C_1: \ 1 \end{align*}$, it is **homogeneous and has a low degree of impurity. 

In chemesity or physics, we use entropy to measure the level of chaos, in computer science, information  or statistics, we use entropy to measure the node's impurity. Here is the formula: 
$$Entropy(t) = -\sum_j p(j|t) log_2 p(j|t) \ \ \ \  Entropy(t)\text{ also written as } H(t)$$
Notice that $p(j|t)$ is the relative frequency of class $j$ at node $t$. Next, we have to measure homogeneity of a node. The maximum $(log \ n_c)$ means that when the records are equally distirbuted among all classes implying least information; The minimum ($0.0)$ means that when all records belong to one class, implying most information. 

![Screenshot 2019-04-14 at 20.04.23.png](https://i.loli.net/2019/04/14/5cb321ccef264.png)

After calculating the Entropy, we have to consider the actrual phenomena, which are multiple classes instead of binary. Therefore, we need to "ungrade" the entropy into **weighted entropy**: $\frac{n_i}{n} Entropy(i)$ where $n_i$ is number of records in partition $i$. Therefore, we can do the splitting based on the **Information Gain**: 
$$GAIN_{split} = Entropy(p) - (\sum_{i = 1}^k \frac{n_i}{n} Entropy(i))$$
Where Parent NOde, $p$ is split into $k$ partitions; $n_i$ is number of records in partion $i$. 

Here is a example: 

![Screenshot 2019-04-14 at 22.35.26.png](https://i.loli.net/2019/04/14/5cb3453384d36.png)

Class P: buys_computer = "yes"
Class N: buys_computer = "no"

At start, all the training example are at root 
$I(yes, no) = I(9, 5) = -\frac{9}{14}log_2(\frac{9}{14}) - \frac{5}{14}log_2(\frac{5}{14}) \approx 0.94$
```{R}
-9/14 * log2(9/14) - 5/14 * log2(5/14)
```

![Screenshot 2019-04-14 at 22.38.46.png](https://i.loli.net/2019/04/14/5cb345fa60361.png)

Then compute the entropy for age: 
$$E(age) = \frac{5}{14}I(2,3) + \frac{4}{14}I(4,0) + \frac{5}{14}I(3,2) = 0.694$$
$$Gain(age) = I(p, n) - E(age) = 0.246$$


### Issue of Tree Induction 

* Determin how to split the records 
  * How to specify the attribute test condition? 
  * How to determine the best split? 
  
* Determin when to stop splitting 

#### How to Specify Test Condition? 

* Depends on attributes types 
  * Nomial 
  * Ordinal 
  * Continuous 
  
* Dependes on number of ways to split 
  * bino-way split: Divides values into two subsets (we need to find optimal partitioning)
  * Multi-way split: Use as many partitions as distinct values 
  
For the continuous attributes, we have to do the discretization to form an ordinal categorical attribution; or we can do the binary decision, which should consider all possible splits and finds the best cut. The binary decision can be more compute intensive. 

#### How to determin the best split 

* Entropy 
* Gini Index 
* Misclassification error 

##### Gini Index 

THe Gini Index is given as: $Gini(t) = 1 - \sum_{j}[p(j|t)]^2$, note that $P(j|t)$ is the relative frequency of class $j$ at node $t$. 

* Maximum $(1-\frac{1}{n_c})$ when records are equally distributed among all classes, implying least interesting information 
* Minimum $(0)$ when all recods belong to one class, implying most interesting information 

$\vdots$ 

```{R}
library(MASS)
data(cats)
head(cats)
library(rpart)
library(rpart.plot)
cats.decision <- rpart(Sex ~., data = cats)
rpart.plot(cats.decision)
cats.predict <- predict(cats.decision, cats, type = "class")
confusion.table <- table(cats.predict, cats$Sex)

accuracy <- function(matrix){
  return (sum(diag(matrix)) / sum(matrix))
}

error <- function(matrix){
  return ((sum(matrix) - sum(diag(matrix))) / sum(matrix))
}

sprintf("The accruacy of this model is %.3f",accuracy(confusion.table)) 
sprintf("THe error of this model is %.3f", error(confusion.table))
```

### Advantages of Decision Tree: 

* Model explains its reasoning -- builds rules 
* Build model quickly 
* Handles non-numeric data (categorical data?)
* No problems with missing data 
  * Missing data as a new value 
  * Surrogate splits 
* Work find with high-dimensions 

### Disadvantages of Decision Tree: 

* Response is always discrete (binned) -- output are step functions 
* Model has high orderinteractions -- all splits are dependent on previous splis 
* Data are split at each node, making futher split able to use less and less data 
* The greedy characteristic of decision tress leads to over sensitivity to the training set, to irrelevant attributes and to oise make decision tress especiall **unstable**: a minor change in one split close to the root will change the whole subtree below.  

## Classification with kNN 

the **k-nearest neighbors algorithm (k-NN) is non-parametric method used for classfication and regression. The basic idea of k-NN is if it walks like a duck, quacks like a duck, then it's probably a duck. 

![Screenshot 2019-04-14 at 23.32.33.png](https://i.loli.net/2019/04/14/5cb35298751af.png)

In k-NN classifier, it requires three things: 
  * The set of stored records
  * Distance Metric to compute distance between records 
  * The value of $k$, the number of neaest neighbors to retrieve 
  
To classify an unknown record: 
  * Compute distance to other training records 
  * Identify $k$ nearest neighbors 
  * Use class labels of nearest neighbors to determin the class label of unknown record (e.g., by taking majority vote)

### Distance of k-NN 
For the Nearest Neighbor Classification, we just need to calculate the nearest point for the train set. Therefore, it also called 1NN. 
* Euclidean distance: $d(p, q) = \sqrt{\sum_i (p_i - q_i)^2}$
* Determine the class from nearest neighbor list 
  * take the majority vote of class labels among the k-nearest neighbors 
Weigh the vote according to distance 
  * weight factor, $w = \frac{1}{d^2}$
## Choosing the value of $k$: 
* If $k$ is too small, sensitive to noise points 
* If $k$ is too large, neighborhood may include points from other classes

**K-NN** classifiers are lazy learners, which means it does not build models explicitly; unlike eager learners such as decision tree induction and rule-based systems and it can classifying unknown records relatively expensive. 

### Example: 
```{R}
# using the data set: iris
train <- rbind(iris3[1:25,,1], iris3[1:25,,2], iris3[1:25,,3])
test <- rbind(iris3[26:50,,1], iris3[26:50,,2], iris3[26:50,,3])
cl <- factor(c(rep("s",25), rep("c",25), rep("v",25)))
attributes(knn(train, test, cl, k = 3, prob=TRUE))

```

## Classification with Naive Bayes / Bayes Series

### Bayes Theorem 

Bayes theorem is a probabilistic framework for solving classification problems, for we are using the conditional probablity and total probability formula
$$\Rightarrow P(C|A) = \frac{P(A|C)P(C)}{P(A)}$$
### Bayesian Classifiers 

Cosider each attribute and class label as random variables, Given a record with attributes $(A_1, A_2, \dots, A_n)$. Our Goal is to predict class $C$. Specifically, we want to find the value of $C$ that maximizes $P(C|A_1, A_2, \dots, A_n)$

Can we estimated $P(C|A_1, A_2, \dots, A_n)$ direactly from the data ? Yes! To approach it, we can compute the posteiro probability $P(C|A_1, A_2, \dots, A_n)$ for all values of $C$ using the Bayes theorem 
$$P(C|A_1 A_2 \dots A_n) = \frac{P(A_1 A_2 \dots A_n)P(C)}{P(A_1 A_2 \dots A_n)}$$
Choose the values of $C$ that can maximiazes $P(C|A_1 A_2 \dots A_n)$ which is equivalent to choosing value of $C$ that maximizes $P(A_1 A_2 \dots A_n | C)$ 

There for we need **Naive Bayes Classifier**. In naive bayes classifier, we assume independence among attributes $A_i$ when class is given: 
$P(A_1, A_2, \dots, A_n|C) = P(A_1|C_j)P(A_2|C_j)\dots P(A_n|C_j)$. We can estimate $P(A_i|C_j)$  for all $A_i$ and $C_j$. 

We assume that there are strong independent between variables and we can estimate them easily.

What's more , we can do Laplace transformation $(P(A_i|C) = \frac{N_{ic} + 1}{N_c + c})$ and m-estimate: $(P(A_i|C) = \frac{N_{ic} + mp}{N_c + m})$ where c: number of classes; p: prior probability; m: parameter.

#### Summary for Naive Bayes
* Robust to isolated noise points 
* Handle missing values by ignoring the instance during probability estimate calculations 
* Robust to irrelevant attributes 
* Independence assumption may not hold for some attributes 
  * Use other techniques such as Bayesian Belief Networks. 
  
Exameple: 
```{R}
# install.packages("naivebayes")
# we can use also library(e1071) to do the naive bayes 
library(naivebayes)

data(iris)
new <- iris[-c(1,2,3)]
# Add one categorical and count variable
set.seed(1)
new$Discrete <- sample(LETTERS[1:3], nrow(new), TRUE) 
set.seed(1)
new$Counts <- c(rpois(50, 1), rpois(50, 2), rpois(50, 10)) 

# Formula interface
nb <- naive_bayes(Species ~ ., usepoisson = TRUE, data = new)
nb

nb2 <- naive_bayes(x = new[-2], y = new[[2]], usepoisson = TRUE)

# Visualize class conditional probability distributions
plot(nb, which = c("Petal.Width", "Discrete"),
     arg.cat = list(color = heat.colors(3)))
```

### Bayesian Belief Networks 

A Bayesian network, Bayes network, belief network, decision network, is a proabbilistic directed acyclic graphical model. In last section, we have mentioned that a strong independence assumption is needed in Naive Bayes Classifier. However, in Bayesian Belief Network, it can wearker the conditionallly independent assumption.

A belief network is defined by two components: 
* Directed acyclic, graph, where each node represents a random variable and each arc represents a prob. Dependence. 
* A belief network that consists of one conditiaon prob-Table (CPT) for each variable.

![Screenshot 2019-04-15 at 10.15.09.png](https://i.loli.net/2019/04/15/5cb3e933cde6f.png)

Bayesian belief network allows a subset of variables conditionally independent 

A graphical model of causal relationships 
  * it can represent **dependency** among the variables 
  * Gives a specification of joint probability distirbution 
    * Nodes: random variables
    * Links: dependency 
    * X, Y are the parents of Z, and Y is the parent of P
    * No dependency between Z and P 
    * Has no loop or cycles 
    ![Screenshot 2019-04-15 at 10.44.21.png](https://i.loli.net/2019/04/15/5cb3f009f05bb.png)
    
#### Conditionally Independence 

* Property 1: A node in a Bayesian network is **conditionally independent of its non-descendants, if parents are known. For example , in the figure above, A is conditionally independent of both B and D 

#### Conditional Prob. Table 

We use conditional Prob.Table to do the computation. Each node is associated with a probablity table. 
* If a node $X$ does not have any parents, then the table contains only the prior probability $p(X)$ 
* If a node $X$ has only one parent, $Y$ then the table contains the conditional probablity $P(X|Y)$ 
* IF a node $X$ has multiple parents, $(Y_1 Y_2 \dots Y_k)$ the table contains the conditional proabality $P(X|Y_1,Y_2,\dots, Y_k)$

Here is an example: 

![Screenshot 2019-04-15 at 11.13.12.png](https://i.loli.net/2019/04/15/5cb3f6cc62ff0.png)

Calculating $P(\text{HD = Yes}) = \sum_{\alpha} \sum_{\beta} P(HD = Yes| E = \alpha, D = \beta)P(E = \alpha, D = \beta) = \sum_{\alpha} \sum_{\beta} P(HD = Yes| E = \alpha, D = \beta)P(E = \alpha) P(D = \beta)  = 0.49$

#### Model Building 

Building a Bayesiaan network model involves two steps: 

1. Create the structure of the network 
2. Estimate the probability values in the tables associated with each node 


  
  